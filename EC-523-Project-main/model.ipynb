{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "128c0f16",
   "metadata": {},
   "source": [
    "## Import modules needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f978a4bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import functools\n",
    "from torch.autograd import Variable\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5631bb85",
   "metadata": {},
   "source": [
    "## Definition of basic functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b298ed53",
   "metadata": {},
   "outputs": [],
   "source": [
    "def weights_init(m):\n",
    "    \"\"\"\n",
    "    This function initialize the weights of neural networks\n",
    "    \"\"\"\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find('Conv') != -1:\n",
    "        nn.init.normal_(m.weight.data, 0.0, 0.02)\n",
    "        if m.bias is not None:\n",
    "            nn.init.constant_(m.bias.data, 0.0)\n",
    "    elif classname.find('BatchNorm2d') != -1:\n",
    "        nn.init.normal_(m.weight.data, 1.0, 0.02)\n",
    "        nn.init.constant_(m.bias.data, 0.0)\n",
    "    elif classname.find('Linear') != -1:\n",
    "        nn.init.normal_(m.weight.data, 0.0, 0.02)\n",
    "        if m.bias is not None:\n",
    "            nn.init.constant_(m.bias.data, 0.0)\n",
    "            \n",
    "def get_norm_layer(norm_type='batch'):\n",
    "    \"\"\"\n",
    "    This function initialize the normalization type of neural networks\n",
    "    \"\"\"\n",
    "    if norm_type == 'batch':\n",
    "        norm_layer = nn.BatchNorm2d\n",
    "    elif norm_type == 'instance':\n",
    "        norm_layer = nn.InstanceNorm2d\n",
    "    elif norm_type == 'layer':\n",
    "        norm_layer = nn.LayerNorm\n",
    "    else:\n",
    "        raise NotImplementedError('Normalization layer [%s] is not implemented' % norm_type)\n",
    "    return norm_layer\n",
    "\n",
    "def print_network(model):\n",
    "    \"\"\"\n",
    "    This function prints the structure and the number of training parameters of neural networks\n",
    "    \"\"\"\n",
    "    num_params = 0\n",
    "    for param in model.parameters():\n",
    "        num_params += param.numel()\n",
    "    print(model)\n",
    "    print(\"The number of parameters: {}\".format(num_params))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d438de14",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdaptiveInstanceNorm2d(nn.Module):\n",
    "    '''\n",
    "    Cited from the \"MaskGAN: Towards Diverse and Interactive Facial Image Manipulation\"\n",
    "    '''\n",
    "    def __init__(self, num_features, eps=1e-5, momentum=0.1):\n",
    "        super(AdaptiveInstanceNorm2d, self).__init__()\n",
    "        self.num_features = num_features\n",
    "        self.eps = eps\n",
    "        self.momentum = momentum\n",
    "        # weight and bias are dynamically assigned\n",
    "        self.weight = None\n",
    "        self.bias = None\n",
    "        # just dummy buffers, not used\n",
    "        self.register_buffer('running_mean', torch.zeros(num_features))\n",
    "        self.register_buffer('running_var', torch.ones(num_features))\n",
    "\n",
    "    def forward(self, x):\n",
    "        assert self.weight is not None and self.bias is not None, \"Please assign weight and bias before calling AdaIN!\"\n",
    "        b, c = x.size(0), x.size(1)\n",
    "        running_mean = self.running_mean.repeat(b)\n",
    "        running_var = self.running_var.repeat(b)\n",
    "\n",
    "        # Apply instance norm\n",
    "        x_reshaped = x.contiguous().view(1, b * c, *x.size()[2:])\n",
    "\n",
    "        out = F.batch_norm(\n",
    "            x_reshaped, running_mean, running_var, self.weight, self.bias,\n",
    "            True, self.momentum, self.eps)\n",
    "\n",
    "        return out.view(b, c, *x.size()[2:])\n",
    "\n",
    "    def __repr__(self):\n",
    "        return self.__class__.__name__ + '(' + str(self.num_features) + ')'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0d1ffd71",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, stride=1, padding=0, norm='none', activation='relu', padtype='zero'):\n",
    "        super(ConvBlock, self).__init__()\n",
    "        \"\"\"\n",
    "        Initialize the padding operation\n",
    "        \"\"\"\n",
    "        if padtype == 'reflection':\n",
    "            self.pad = nn.ReflectionPad2d(padding)\n",
    "        elif padtype == 'replication':\n",
    "            self.pad = nn.ReplicationPad2d(padding)\n",
    "        elif padtype == 'zero':\n",
    "            self.pad = nn.ZeroPad2d(padding)\n",
    "        elif padtype == 'constant':\n",
    "            self.pad = nn.ConstantPad2d(padding)\n",
    "        else:\n",
    "            assert 0, \"Wrong choice of padding type!\"\n",
    "            \n",
    "        \"\"\"\n",
    "        Initialize the Normalization Layer\n",
    "        \"\"\"\n",
    "        if norm == 'Bn':\n",
    "            self.norm = nn.BatchNorm2d(out_channels)\n",
    "        elif norm == 'Lbn':\n",
    "            self.norm = nn.LazyBatchNorm2d(out_channels)\n",
    "        elif norm == 'In':\n",
    "            self.norm = nn.InstanceNorm2d(out_channels)\n",
    "        elif norm == 'Lin':\n",
    "            self.norm = nn.LazyInstanceNorm2d(out_channels)\n",
    "        elif norm == 'Adain':\n",
    "            self.norm = AdaptiveInstanceNorm2d(out_channels)\n",
    "        elif norm == 'none':\n",
    "            self.norm = None\n",
    "        else:\n",
    "            assert 0, \"Wrong choice of Normalization Layer!\"\n",
    "        \n",
    "        \"\"\"\n",
    "        Initialize the Activation Layer\n",
    "        \"\"\"\n",
    "        if activation == 'ReLU':\n",
    "            self.activation = nn.ReLU(inplace=True)\n",
    "        elif activation == 'Sigmoid':\n",
    "            self.activation = nn.Sigmoid()\n",
    "        elif activation == 'Tanh':\n",
    "            self.activation = nn.Tanh()\n",
    "        elif activation == 'Softmax':\n",
    "            self.activation = nn.Softmax(dim=1)\n",
    "        elif activation == 'LeakyReLU':\n",
    "            self.activation = nn.LeakyReLU(0.2, inplace=True)\n",
    "        elif activation == 'none':\n",
    "            self.activation = None\n",
    "        else:\n",
    "            assert 0, \"Wrong choice of Activation Layer!\"\n",
    "            \n",
    "        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size, stride, bias=True)        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.conv(self.pad(x))\n",
    "        if self.norm:\n",
    "            x = self.norm(x)\n",
    "        if self.activation:\n",
    "            x = self.activation(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "67e9ca33",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SFT(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SFT, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(64, 64, 1)\n",
    "        self.conv2 = nn.Conv2d(64, 64, 1) \n",
    "        self.conv3 = nn.Conv2d(64, 64, 1)\n",
    "        self.conv4 = nn.Conv2d(64, 64, 1)\n",
    "\n",
    "    def forward(self, x, y):\n",
    "        '''\n",
    "        x is the feture map\n",
    "        y is the conditions\n",
    "        '''\n",
    "        gamma = self.conv2(F.leaky_relu(self.conv1(y), 0.1, inplace=True))\n",
    "        beta = self.conv4(F.leaky_relu(self.conv3(y), 0.1, inplace=True))\n",
    "        return x * gamma + beta"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97c58e19",
   "metadata": {},
   "source": [
    "## Implementation of the Spatial Style Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8027cfe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SpatialStyleEncoder(nn.Module):\n",
    "    def __init__(self, num_adain_para):\n",
    "        super(SpatialStyleEncoder, self).__init__()\n",
    "        self.label_conv_1 = ConvBlock(19, 16, 7, 1, 3)\n",
    "        self.label_conv_2 = ConvBlock(16, 32, 4, 2, 1)\n",
    "        self.label_conv_3 = ConvBlock(32, 64, 4, 2, 1, activation='none')\n",
    "        self.label_conv_4 = ConvBlock(64, 64, 4, 2, 1)\n",
    "        self.label_conv_5 = ConvBlock(64, 64, 4, 2, 1)\n",
    "        self.label_conv_6 = ConvBlock(32, 64, 4, 2, 1, activation='none')\n",
    "        self.style_conv_1 = ConvBlock(3, 16, 7, 1, 3)\n",
    "        self.style_conv_2 = ConvBlock(16, 32, 4, 2, 1)\n",
    "        self.style_conv_3 = ConvBlock(32, 64, 4, 2, 1)\n",
    "        self.sft_layer_1 = SFT()\n",
    "        self.style_conv_4 = ConvBlock(64, 64, 4, 2, 1)\n",
    "        self.style_conv_5 = ConvBlock(64, 64, 4, 2, 1)\n",
    "        self.style_conv_6 = ConvBlock(64, 64, 4, 2, 1)\n",
    "        self.sft_layer_2 = SFT()\n",
    "        self.average_pool = nn.AdaptiveAvgPool2d(1)\n",
    "        self.conv_last = nn.Conv2d(64, num_adain_para, 1, 1, 0)\n",
    "    \n",
    "    def forward(self, x, y):\n",
    "        '''\n",
    "        x is the labed mask picture\n",
    "        y is the origin picture\n",
    "        '''\n",
    "        y_out_1 = self.label_conv_1(y)\n",
    "        y_out_1 = self.label_conv_2(y_out_1)\n",
    "        y_out_1 = self.label_conv_3(y_out_1)\n",
    "        y_out_2 = self.label_conv_4(y_out_1)\n",
    "        y_out_2 = self.label_conv_5(y_out_2) \n",
    "        y_out_2 = self.label_conv_6(y_out_2) \n",
    "        x = self.style_conv_1(x)\n",
    "        x = self.style_conv_2(x)  \n",
    "        x = self.style_conv_3(x)\n",
    "        x = self.sft_layer_1(x, y_out_1)\n",
    "        x = self.style_conv_4(x)\n",
    "        x = self.style_conv_5(x)  \n",
    "        x = self.style_conv_6(x)\n",
    "        x = self.sft_layer_2(x, y_out_2)\n",
    "        x = nn.AdaptiveAvgPool2d(1)\n",
    "        out = self.conv_last(x)\n",
    "\n",
    "        return out"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
